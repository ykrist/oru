#!/usr/bin/env python
import csv
import os
import argparse
import sys
import itertools
import fnmatch
import json
import signal
import re

from oru import map_keys
from typing import Dict, Tuple, Any
from functools import reduce

# sys.stdin = open('scrap.json', 'r')

NestedDict = Dict[str,Dict]
Key = str
TupleKey = Tuple[Key,...]
FlatDict = Dict[TupleKey, Any]

# Make python simply stop when piping into head/tail (SIG_DFL is the "default" signal - i.e. don't let python
# handle SIGPIPE and throw and exception).
signal.signal(signal.SIGPIPE, signal.SIG_DFL)

class ParseError(Exception):
    def __init__(self, msg):
        self.msg = msg
        super().__init__()


def parse_json_blobs(s : str):
    depth = 0
    ends = []
    starts = []
    for idx, c in enumerate(s):
        if c == "{":
            if depth == 0:
                starts.append(idx)
            depth += 1
        elif c == "}":
            depth -= 1
            if depth == 0:
                ends.append(idx)
        elif re.match(r'\S', c) and depth == 0:
            raise ParseError("nonwhitespace character between blobs: " + c)

        if depth < 0:
            raise ParseError("bad format: unmatched `}`")

    if depth > 0:
        raise ParseError("bad format: unmatched `{`")

    blobs = []
    for i,j in zip(starts, ends):
        blobs.append(json.loads(s[i:j+1]))

    return blobs


def convert_filename(fn, args):
    if not args.kd:
        fn = os.path.basename(fn)
    if not args.ke:
        fn, _ = os.path.splitext(fn)
    return fn


def error(*args, exit=True, **kwargs):
    kwargs['file'] = sys.stderr
    print('error:', *args, **kwargs)
    if exit:
        sys.exit(1)

def recursive_merge(d1 : NestedDict, d2 : NestedDict) -> NestedDict:
    """Merge dictionaries d1 and d2 recursively.  Values from d2 take precedence."""
    new = d1.copy()

    for k,v1 in d1.items():
        if k in d2:
            v2 = d2[k]
            if isinstance(v1, dict) and isinstance(v2, dict):
                new[k] = recursive_merge(v1,v2)
            else:
                new[k] = v2

    for k,v in d2.items():
        if k not in new:
            new[k] = v

    return new


def flatten_dictionary(d : NestedDict, _prefix=()) -> FlatDict:
    new_d = {}
    for key in d:
        new_key = _prefix + (key,)
        if isinstance(d[key], dict) and len(d[key]) > 0:
            new_d.update(flatten_dictionary(d[key], new_key))
        else:
            new_d[new_key] = d[key]
    return new_d

def unflatten_dictionary(d : FlatDict) -> NestedDict:
    new_d = {}
    for reckey in d:
        current_d = new_d
        for key in reckey[:-1]:
            if key not in current_d:
                current_d[key] = {}
            current_d = current_d[key]
        current_d[reckey[-1]] = d[reckey]
    return new_d

def expand_tuplekeys(d : FlatDict, sep : str) ->  FlatDict:
    return map_keys(lambda tuplekey : tuple(k for key in tuplekey for k in key.split(sep)), d)

def join_tuplekeys(d : FlatDict, sep : str) -> Dict[str, Any]:
    return map_keys(lambda tuplekey : sep.join(tuplekey),d)

def main(args):
    if args.input.count('-') > 1:
        error('STDIN given multiple times.')

    data = []
    input_files = []
    for i,f in enumerate(args.input):
        if f == '-':
            stdin = sys.stdin.read()
            try:
                d = json.loads(stdin)
                if isinstance(d, dict):
                    d = [d]
            except json.JSONDecodeError:
                try:
                    d = parse_json_blobs(stdin)
                except ParseError as e:
                    error("unable to parse JSON blobs from STDIN: "+ e.msg)
                except json.JSONDecodeError:
                    error("unable to parse JSON blobs from STDIN: bad JSON format.")

            if len(d) > 1:
                input_files.extend([f"STDIN{i:d}" for i in range(len(d))])
            elif len(d) == 1:
                input_files.append("STDIN")
            data.extend(d)

        else:
            with open(f, 'r') as fp:
                data.append(json.load(fp))
            input_files.append(f)

    if args.insert is not None:
        if len(data) <= 1:
            error("At least 2 inputs are required.")
        elif len(data)  != len(args.insert) + 1:
            error("Missing/extra field names: Number of field names ({}) should be one less than"
                  " the number of inputs ({}).".format(len(args.insert), len(data)))
        args.insert.extend(map(lambda fn : convert_filename(fn, args), input_files[len(args.insert)+1:]))

    if args.merge:
        data = [reduce(recursive_merge, data)]
    elif args.insert is not None:
        for i in reversed(range(1, len(data))):
            data[0][args.insert[i-1]] = data.pop(-1)

    if args.merge or args.insert:
        input_files = ['MERGED']

    if args.print_field:
        if len(data) > 1:
            error("Can only print from a single input (or multiple inputs that have been merged).")

    if args.single_line:
        json_kwargs = {}
    else:
        json_kwargs = {'indent' : '\t'}


    if args.csv or args.flatten or args.patterns or args.unflatten:
        for i in range(len(data)):
            data[i] = flatten_dictionary(data[i])

    if args.unflatten:
        for i in range(len(data)):
            data[i] = expand_tuplekeys(data[i], args.level_sep)

    if args.patterns:
        patterns = tuple(map(lambda x: x.split(args.level_sep), args.patterns))
        for i in range(len(data)):
            for key in list(data[i].keys()):
                for pattern in patterns:
                    if all(fnmatch.fnmatch(k, p) for k, p in zip(key, pattern)):
                        break
                else:
                    del data[i][key]


    if args.csv or args.flatten:
        for i in range(len(data)):
            data[i] = join_tuplekeys(data[i], args.level_sep)
    elif args.patterns or args.unflatten:
        for i in range(len(data)):
            data[i] = unflatten_dictionary(data[i])

    if args.output is not None:
        fp = open(args.output, 'w')
        closefile = True
    else:
        fp = sys.stdout
        closefile = False

    if args.csv:
        csv_fields = set(itertools.chain(*map(lambda r: r.keys(), data)))

        index_field_name = 'index'
        index_suffix = 0
        while index_field_name in csv_fields:
            index_suffix += 1
            index_field_name = f'index{index_suffix:d}'

        csv_fields = [index_field_name] + sorted(csv_fields)
        csv_writer = csv.DictWriter(fp, csv_fields, extrasaction='ignore')
        csv_writer.writeheader()

        for r, fname in zip(data, input_files):
            r[index_field_name] = convert_filename(fname, args)
            csv_writer.writerow(r)
            fp.flush()

    elif args.print_field is not None:
        tuplekey = args.print_field.split(args.level_sep)
        d = data[0]
        for i,k in enumerate(tuplekey):
            try:
                d = d[k]
            except KeyError:
                error('Field not found: ' + ' -> '.join(['<root>'] + tuplekey[:i+1]))

        if isinstance(d, dict):
            d = json.dumps(d, **json_kwargs) + '\n'
        else:
            d = str(d) + '\n'
        fp.write(d)

    else:
        if len(data) == 1:
            data = data[0]
        json.dump(data ,fp, **json_kwargs)
        fp.write('\n')

    if closefile:
        fp.close()


if __name__ == '__main__':
    p = argparse.ArgumentParser(add_help=False)
    p.add_argument("input", type=str, nargs='*', default=['-'],
                   help='Input JSON files, use - for STDIN. If the top-level JSON object to STDIN is an array,'
                        ' each element in this array will be treated as a separate JSON input.  Useful for chaining '
                        '%(prog)s commands together in a pipeline. '
                        'Alternatively, when using a STDIN, a sequence of JSON objects may be supplied without '
                        'delimiters, eg such as `cat *.json | json-util`. Each JSON object will be treated as a separate '
                        'input.  Default is STDIN.')

    generaloptions = p.add_argument_group("general options")
    generaloptions.add_argument("-h","--help", action='help')
    generaloptions.add_argument("-l","--level-sep",type=str,default='.',metavar='SEP',
                   help="Separator to use to identifying JSON levels when producing flattening/unflattening, "
                        "and specifying. Default is `%(default)s`")
    generaloptions.add_argument("-ke", action="store_true",
                   help="Keep file extension when converting input filenames")
    generaloptions.add_argument("-kd", action="store_true",
                   help="Keep directories when converting input filenames")
    generaloptions.add_argument("-f", "--filter", type=str, dest='patterns', action='append',
                   help="Select fields from JSON file(s).  Use SEP (see --level-sep) to access nested fields.  "
                         "Standard wildcard matching is supported.  Use this option multiple times to specify"
                         " several patterns at once")
    generaloptions.add_argument("-s", "--single-line", action='store_true',
                                help='Print JSON output on a single line.')

    preprocessing = p.add_argument_group("preprocessing options")
    preprocessing = preprocessing.add_mutually_exclusive_group()
    preprocessing.add_argument("-i", "--insert", nargs='*', type=str, metavar="FIELD",
                   help="Insert all JSON files after the first, into the first.  "
                        "For each input JSON file after the first, a JSON field name may be supplied.")
    preprocessing.add_argument("-m", "--merge", action='store_true',
                   help= "Merge input files recursively before any operations.")


    outputcontrol = p.add_argument_group("output control",
                                         description="The default output is JSON to STDOUT, multiple outputs are"
                                                     " concatenated into a JSON array of objects.")
    outputcontrol.add_argument("-o", "--output", type=str, default=None,
                   help="Save the result to a file rather than printing to STDOUT.")
    outputcontrol = outputcontrol.add_mutually_exclusive_group()
    outputcontrol.add_argument("--flatten", action='store_true',
                   help="Produce JSON output that is exactly 1 level deep.")
    outputcontrol.add_argument("--unflatten", action='store_true',
                   help="Convert flattened JSON to nested JSON (inverse of --flatten).")
    outputcontrol.add_argument("--csv", action='store_true',
                   help="Convert into a CSV file, where each JSON output is one row in the output.  "
                        "Nested JSON keys will be converted to column names in the same manner as `--flatten`.")
    outputcontrol.add_argument("-p", "--print-field", type=str,
                                help='Print a single field.  Use SEP (see --level-sep) to access nested fields.')

    args = p.parse_args(sys.argv[1:])
    main(args)
