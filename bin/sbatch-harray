#!/usr/bin/env python
import oru.slurm
import argparse
import subprocess
import sys
import tempfile
import os
import asyncio
from oru import frozendict
import re
import shutil
import shelve
import glob
from typing import Dict, List, Tuple

DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)),'.sbatch-harray.data.d')
DATABASE_FILE = os.path.join(DATA_DIR, 'db')
SLURM_LOG_DIR = os.path.join(DATA_DIR, 'logs')

SBATCH_COMMAND = ['sbatch', '--parsable']

SLURM_INFO_AGGREGATABLE = {
    'mem',
    'cpus-per-task',
    'tasks-per-node',
    'nodes',
    'mail-user',
    'mail-type',
    'constraint',
    'time',
    'script'
}

SLURM_INFO_NOT_AGGREGATABLE = {
    'job-name',
    'out',
    'err'
}


def ensure_directories_exist():
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(SLURM_LOG_DIR, exist_ok=True)


def panic(msg):
    print(msg, file=sys.stderr)
    sys.exit(1)

class SBHError(Exception):
    def __init__(self, msg):
        self.msg = msg
        super().__init__(msg)


def format_indices(indices : list):
    indices = sorted(indices)
    expr = []
    start_range = indices[0]
    end_range = indices[0]
    def _fmt_range(s, e):
        if s < e:
            return f'{s}-{e}'
        else:
            return str(s)

    for i in indices[1:]:
        if i == end_range + 1:
            end_range = i

        else:
            expr.append(_fmt_range(start_range,end_range))
            start_range = i
            end_range = i

    expr.append(_fmt_range(start_range,end_range))
    return ','.join(expr)

async def retrieve_slurm_info(cl_args):
    sem = asyncio.Semaphore(256) # limit to 256 child procs
    async def get_info(i):
        command = ["python", cl_args.target, '--slurminfo'] + cl_args.target_args + [str(i)]
        async with sem:
            proc = await asyncio.create_subprocess_exec(*command, stdout=asyncio.subprocess.PIPE)
            slurm_json, _ = await proc.communicate()
            if proc.returncode != 0:
                raise SBHError(f"failed to get slurm infomation: TARGET return exit code: {proc.returncode:d}")
            else:
                try:
                    slurm_json = oru.slurm.parse_slurm_info(slurm_json.decode())
                except Exception as e:
                    raise SBHError(f"failed to parse slurm information: " + str(e))

        return slurm_json

    return await asyncio.gather(*[get_info(i) for i in cl_args.array_range])

class SlurmArrayJob:
    def __init__(self, sbatch_opts : Dict[str,str], script_preamble : str):
        self.indices =  []
        self.sbatch_opts = sbatch_opts
        self.script_preamble = script_preamble
        self.index_to_name = {}
        self.index_to_logs = {}
        self.index_cmd = {}

    def add_job(self, index : int, name : str, out : str, err : str, cmd : List[str]):
        self.indices.append(index)
        self.index_cmd[index] = cmd
        self.index_to_name[index] = name
        self.index_to_logs[index] = (os.path.abspath(out), os.path.abspath(err))

    def get_job_script(self):
        script = self.script_preamble + '\ncase $SLURM_ARRAY_TASK_ID in\n'
        for i,command in self.index_cmd.items():
            script += '{})\n\t{}\n;;\n'.format(str(i), " ".join(command))
        script += 'esac\n'
        return script

    def get_sbatch_args(self):
        sbatch_args = []
        for key, val in self.sbatch_opts.items():
            sbatch_args.extend(("--" + key, val))
        sbatch_args.extend(("--array", format_indices(self.indices)))
        sbatch_args.extend(("--out", os.path.join(SLURM_LOG_DIR, '%A_%a.out')))
        sbatch_args.extend(("--err", os.path.join(SLURM_LOG_DIR, '%A_%a.err')))
        return sbatch_args

def command_submit(args):
    if shutil.which('sbatch') is None:
        print('warning: `sbatch` could not be found, using `sbatch-fake` instead.', file=sys.stderr)
        SBATCH_COMMAND[0] = 'sbatch-fake'

    array_jobs : Dict[frozendict, SlurmArrayJob] = {}
    try:
        index_slurm_info = dict(zip(args.array_range, asyncio.run(retrieve_slurm_info(args))))
    except SBHError as e:
        panic(e.msg)

    for i,slurm_info in index_slurm_info.items():
        array_job_id = frozendict((k,slurm_info.pop(k)) for k in list(slurm_info.keys())
                                  if k in SLURM_INFO_AGGREGATABLE)
        if array_job_id not in array_jobs:
            sbatch_opts = dict(array_job_id)
            script_preamble = sbatch_opts.pop('script')
            array_jobs[array_job_id] = SlurmArrayJob(sbatch_opts, script_preamble)

        cmd = ["python", "-O", args.target] + args.target_args + [str(i)]

        array_jobs[array_job_id].add_job(i, slurm_info.get('job-name', 'null'),
                                         slurm_info['out'], slurm_info['err'], cmd)

    with shelve.open(DATABASE_FILE) as db:
        for array_job in array_jobs.values():
            bash_script_contents = array_job.get_job_script()
            bash_script_file = tempfile.NamedTemporaryFile(mode='w', delete=False)
            bash_script_file.file.write(bash_script_contents)
            bash_script_file.file.close()

            command = SBATCH_COMMAND + array_job.get_sbatch_args()
            if args.dryrun:
                command.append('--test-only')
            command.append(bash_script_file.name)

            if args.verbose:
                print(" ".join(command))
                print(bash_script_file.name.center(80, '-'))
                print(bash_script_contents)
                print("-" * 80)

            result = subprocess.run(command, stdout=subprocess.PIPE, text=True)

            if result.returncode != 0:
                os.remove(bash_script_file.name)
                panic(f"command `{' '.join(command)}` exit with nonzero status {result.returncode:d}:")

            if not args.dryrun:
                job_id = result.stdout.strip()
                for i,n in array_job.index_to_name.items():
                    out,err = array_job.index_to_logs[i]
                    db[f'{job_id}_{i}'] = {'name' : n, 'err' : err, 'out' : out}
                db.sync()
                print(f'Submitted job ID {job_id}')

            os.remove(bash_script_file.name)


def command_name(args):
    with shelve.open(DATABASE_FILE) as db:
        keys = db.keys()

        if len(args.jobid) == 0:
            matches = keys
        elif args.regexp:
            matches = []
            for s in keys:
                for p in args.jobid:
                    if re.search(p,s) is not None:
                        matches.append(s)
                        break

        else:
            matches = args.jobid

        for i in matches:
            try:
                print(i, db[i]['name'])
            except KeyError:
                panic(f'job id not found: {i}')

def command_clear(args):
    if len(glob.glob(os.path.join(SLURM_LOG_DIR, '*.out'))) > 0:
        panic('pending logs still exist, run `sbatch-harray log` first.')

    for f in glob.glob(DATABASE_FILE + '.*'):
        os.remove(f)
    print('database cleared.')

def command_log(args):
    with shelve.open(DATABASE_FILE) as db:
        for out in glob.glob(os.path.join(SLURM_LOG_DIR, '*.out')):
            slurm_id = os.path.basename(out).rstrip('.out')
            err = out.rstrip('.out') + '.err'
            if not os.path.exists(err):
                panic(f'{slurm_id} has a STDOUT file but not STDERR')

            db_row = db[slurm_id]
            out_dest = db_row['out']
            err_dest = db_row['err']
            for src,dest in [(out,out_dest), (err, err_dest)]:
                try:
                    shutil.copyfile(src,dest)
                except FileNotFoundError:
                    panic(f'Missing directory: ' + os.path.dirname(dest))

            os.remove(out)
            os.remove(err)


if __name__ == '__main__':
    ensure_directories_exist()

    subcommands = ('submit', 'name', 'clear', 'log')
    argv = sys.argv.copy()[1:]
    if len(argv) > 0:
        if argv[0] not in subcommands and argv[0] not in ('-h', '--help'):
            argv = ['submit'] + argv

    p = argparse.ArgumentParser(allow_abbrev=False)
    subparsers = p.add_subparsers()

    submit = subparsers.add_parser('submit',help='Submit a heterogenous array job (default).')
    submit.set_defaults(func=command_submit)
    submit.add_argument("-v", "--verbose", action='store_true')
    submit.add_argument("--dryrun", action='store_true',
                   help="Don't submit anything.  Useful for debugging.")
    submit.add_argument("target",
                   help="target file to run.  The TARGET must accept a --slurmid switch, and when given this option, "
                        "should print a JSON string to STDOUT containing all the necessary information before "
                        "exiting.  Furthermore, the last positional argument of TARGET must be an integer; "
                        "this is what is passed to the TARGET based on the "
                        "supplied ARRAY_RANGE.  Thus TARGET must support usage as follows: "
                        "`target --slurmid [target_args]"
                        "idx`.")
    submit.add_argument("target_args", nargs="*",
                   help="Arguments to pass through to TARGET.  If optional arguments are being passed to TARGET "
                        "(arguments beginning with `-`, then this list must be prefixed with `--`.")
    submit.add_argument("array_range",
                   help="Array indices to run over, each array index is passed separately to TARGET.",
                   type=oru.slurm.array_range)


    name = subparsers.add_parser('name', help='Get the name of a `job-name` by its job id')
    name.set_defaults(func=command_name)
    name.add_argument('jobid', type=str, nargs='*', metavar='JOBID', default=[],
                      help='Slurm job IDs to filter by.  If none are provided, all entries are printed.')
    name.add_argument('-r', '--regexp', action='store_true', help='Treat JOBID as Perl-compatible regular expressions.')

    clear = subparsers.add_parser('clear', help='Clear all database entries.')
    clear.set_defaults(func=command_clear)

    log = subparsers.add_parser('log', help='Push pending log files to their destinations.')
    log.set_defaults(func=command_log)

    if len(argv) == 0:
        p.print_usage()
        sys.exit(1)
    else:
        args = p.parse_args(argv)
        args.func(args)
